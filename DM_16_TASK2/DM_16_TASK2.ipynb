{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPHfyRbNzKcv"
   },
   "source": [
    "# Clustering analysis\n",
    "\n",
    "**Authors:** \n",
    "- Marc Villalonga Llobera\n",
    "- Patxi Juaristi Pagegi\n",
    "\n",
    "**Date:** 08/01/2024\n",
    "\n",
    "---\n",
    "\n",
    "This Jupyter Notebook covers the second task of the project for the Data Mining subject of the Laurea Magistrale of the University of Pisa, focused in clustering analysis.\n",
    "\n",
    "Clustering analysis is a powerful data exploration technique that aims to group similar data points together based on certain features or characteristics. It plays a crucial role in uncovering underlying patterns and structures within large datasets, allowing for the identification of distinct subsets or clusters. In this analysis, we employ clustering algorithms to discern natural groupings within our datasets, providing valuable insights into the inherent relationships among data points. This method enhances our understanding of complex datasets by revealing patterns that may not be immediately apparent, thereby facilitating more targeted and informed decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwQ_z05hzsLh"
   },
   "source": [
    "## Environment preparation and data reading\n",
    "\n",
    "First of all, we will install all the required packages, and then import the libraries that we will use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfOUKxJsznAp",
    "outputId": "efba5e14-dcaf-4881-fa82-c4351994e134"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!python -m pip install --upgrade pip\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "#!pip install seaborn\n",
    "#!pip install scipy\n",
    "#!pip install scikit-learn\n",
    "#!pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from kneed import KneeLocator\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the required libraries, we will read the datasets that we have exported in the task 1, which they contain the data filtered after the data preparation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three datasets\n",
    "incidents_dataset = pd.read_csv('../project_datasets/incidents_v2.csv', low_memory=False)\n",
    "pbsy_dataset = pd.read_csv('../project_datasets/povertyByStateYear_v2.csv')\n",
    "ysdh_dataset = pd.read_csv('../project_datasets/year_state_district_house_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Analysis by K-means on the entire dataset:\n",
    "\n",
    "First clustering approach that we will use will be the clustering analysis by K-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identification of the best k value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we perform K-means clustering on the `incidents_dataset` after dropping rows with missing values and selecting numeric features for clustering. The selected features are standardized using `StandardScaler`, and the optimal number of clusters (`k`) is determined using `KneeLocator` from `kneed` library, which employs the Elbow Method to do it. In this case, we get that the optimal `k` value is 3. After calculating it, a plot is generated to visualize the \"elbow\" point, aiding in the identification of the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data_for_clustering = incidents_dataset.dropna()\n",
    "\n",
    "# Select all features for clustering\n",
    "selected_features = data_for_clustering.columns\n",
    "\n",
    "# Exclude non-numeric columns (e.g., 'object' type columns)\n",
    "selected_features = data_for_clustering.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "data_for_clustering_standardized = scaler.fit_transform(data_for_clustering[selected_features])\n",
    "\n",
    "# Determine the optimal number of clusters (k) using the elbow method\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)  # Explicitly set n_init\n",
    "    kmeans.fit(data_for_clustering_standardized)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Find the optimal k using the KneeLocator from kneed library\n",
    "knee = KneeLocator(range(1, 11), inertia, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_k = knee.elbow\n",
    "\n",
    "# Print the optimal k\n",
    "print(f\"Optimal k value: {optimal_k}\")\n",
    "\n",
    "# Plot the elbow method with the optimal k\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.vlines(optimal_k, plt.ylim()[0], plt.ylim()[1], linestyles='dashed', colors='red', label='Optimal k')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster generation and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After identifying the best k value for clustering, we start performing k-means clustering on the standardized data.\n",
    "\n",
    "The algorithm assigns each data point to a cluster and adds the cluster labels as a new column (`cluster`) to the original dataset. After clustering, we calculate the centroids of each cluster and transforms them back to the original scale using the scaler. The resulting centroids are then printed, providing insights into the representative values of the features for each cluster.\n",
    "\n",
    "These centroids represent the average values of various features within each cluster resulting from the k-means clustering. Each row corresponds to one cluster, and the columns display the centroids of different features. For instance, in the 'latitude' and 'longitude' columns, the values represent the average geographic coordinates of incidents within each cluster. Similarly, the 'participant_age1' column shows the average age of participants in each cluster. The 'cluster' column indicates the assigned cluster label for each row, and therefore, the presence of three rows indicates that there are three clusters (0, 1, and 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering with the optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, n_init=10, random_state=42)  # Explicitly set n_init\n",
    "clusters_kmeans = kmeans.fit_predict(data_for_clustering_standardized)\n",
    "\n",
    "# Add clusters to the original dataset\n",
    "incidents_dataset['cluster'] = pd.Series(clusters_kmeans, index=data_for_clustering.index)\n",
    "\n",
    "# Analysis of k centroids\n",
    "centroids = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=selected_features)\n",
    "print(\"Centroids:\")\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the centroids and the clusters, we perform a comparison of variable distributions within each cluster. For each cluster (identified by the cluster label), we print descriptive statistics: mean, standard deviation, minimum, 25th percentile, median, 75th percentile, and maximum. Then we create histograms for the three clusters, to visualize the distribution of each variable within the specific cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of variable distributions within clusters\n",
    "for cluster_label in range(optimal_k):\n",
    "    print(f\"\\nCluster {cluster_label} Statistics:\")\n",
    "    cluster_data = incidents_dataset[incidents_dataset['cluster'] == cluster_label][selected_features]\n",
    "    cluster_stats = cluster_data.describe().transpose()\n",
    "    print(cluster_stats)\n",
    "\n",
    "    # Plot variable distributions within the cluster\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for column in selected_features:\n",
    "        plt.hist(cluster_data[column], alpha=0.5, label=column)\n",
    "    plt.title(f\"Variable Distributions within Cluster {cluster_label}\")\n",
    "    plt.xlabel(\"Values\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the obtained three cluster in order to define which is the best. To achieve that we will use two different techniques:\n",
    "\n",
    "- **Silhouette Score**: This approach measures how well-separated the clusters are. A value close to 1 indicates well-defined, distinct clusters, while a score near 0 suggests overlapping clusters. In out case, a score of 0.213 is reasonable. It indicates moderate separation between clusters. However, the clusters may not be perfectly well-defined.\n",
    "- **Inertia**: It measures the compactness of clusters. It is the sum of squared distances between data points and their assigned cluster center. Lower inertia values indicate more compact clusters. Our inertia value is relatively high, suggesting that the clusters are not very compact. This might indicate that the clusters are spread out or not as tightly grouped as they could be.\n",
    "\n",
    "Interpreting these metrics together, we can consider that the Silhouette Score suggests moderate separation between clusters, but not highly distinct, while the relatively high Inertia value indicates that the clusters are not very compact. In summary, while the clustering is providing some separation, there is room for improvement, so it could be interesting to test other clustering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Silhouette Score\n",
    "silhouette_avg = silhouette_score(data_for_clustering_standardized, clusters_kmeans)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Calculate Inertia\n",
    "print(f\"Inertia: {kmeans.inertia_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by density-based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test another clustering approach, we will do a analysis by density-based clustering. In this case, we will not use the entire dataset, but a filtered dataset by state. For our test, we will use California as the testing state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for the state of California\n",
    "california_data = incidents_dataset[incidents_dataset['state'] == 'CALIFORNIA'].copy()\n",
    "\n",
    "# Drop rows with missing values\n",
    "california_data = california_data.dropna()\n",
    "\n",
    "# Select features for clustering\n",
    "selected_features = california_data.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study of the clustering parameters\n",
    "\n",
    "After having defined the dataset with a unique state, we start with the density-based clustering analysis using DBSCAN. We first standardize the selected features of the dataset for the state of California using `StandardScaler`. Then, we calculate the distance to the k<sup>th</sup> nearest neighbor for each data point and visualize the resulting k-distance graph. The code utilizes the `KneeLocator` algorithm to identify the optimal epsilon (neighborhood distance) by finding the knee point in the graph. In our case, he optimal epsilon value is around 5'1. In the final step we plot the k-distance graph with the identified optimal epsilon marked in red. This process helps determine a suitable parameter for the DBSCAN algorithm when clustering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "california_data_standardized = scaler.fit_transform(california_data[selected_features])\n",
    "\n",
    "# Determine the optimal eps (neighborhood distance) using k-distance graph\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(california_data_standardized)\n",
    "distances, indices = nbrs.kneighbors(california_data_standardized)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:, 1]\n",
    "\n",
    "# Find the optimal epsilon using the KneeLocator\n",
    "kneedle = KneeLocator(range(len(distances)), distances, S=1.0, curve=\"convex\", direction=\"increasing\")\n",
    "optimal_eps_index = kneedle.knee\n",
    "optimal_eps = distances[optimal_eps_index]\n",
    "\n",
    "# Print the optimal epsilon\n",
    "print(f\"Optimal Epsilon: {optimal_eps}\")\n",
    "\n",
    "# Plot the k-distance graph with the identified knee point\n",
    "plt.plot(distances)\n",
    "plt.scatter(optimal_eps_index, optimal_eps, c='red', label='Optimal Epsilon')\n",
    "plt.xlabel('Data Points Sorted by Distance')\n",
    "plt.ylabel('Epsilon (Distance to the kth Nearest Neighbor)')\n",
    "plt.title('K-Distance Graph with Optimal Epsilon')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterization and interpretation of the obtained clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the optimal epsilon value deined, we perform the Density-Based Spatial Clustering of Applications with Noise (DBSCAN). The DBSCAN algorithm classifies data points into clusters based on their density within defined neighborhoods. The `min_samples` parameter specifies the minimum number of data points required to form a dense region. The resulting clusters are visualized in a scatter plot, where each point represents an incident. The plot uses `avg_age_participants` and `n_participants` as axes, and different colors represent distinct clusters identified by DBSCAN.\n",
    "\n",
    "From the results obtained, we can see that the clustering creates two clusters, which, looking at the results, we can affirm that they are differentiated between the main trend and the less frequent cases. That is, the yellow dots in the graph represent the most common incidents, which are those with the lowest number of participants. On the contrary, the incidents with the highest number of participants are included in the second cluster, represented by the purple dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering with the optimal epsilon\n",
    "dbscan = DBSCAN(eps=optimal_eps, min_samples=5)\n",
    "clusters_dbscan = dbscan.fit_predict(california_data_standardized)\n",
    "\n",
    "# Add clusters to the original dataset\n",
    "california_data['cluster'] = pd.Series(clusters_dbscan, index=california_data.index)\n",
    "\n",
    "num_clusters = california_data['cluster'].nunique()\n",
    "print(f\"Number of Clusters: {num_clusters}\")\n",
    "\n",
    "# Visualize the clusters (replace 'avg_age_participants' and 'n_killed' with actual column names)\n",
    "plt.scatter(california_data['avg_age_participants'], california_data['n_participants'], c=california_data['cluster'], cmap='viridis')\n",
    "plt.xlabel('Average Age of Participants')\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.title('Clusters based on Average Age and Number of Participants (DBSCAN)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by hierarchical clustering\n",
    "\n",
    "To conclude with the clustering analysis, we implement an analysis by hierarchical clustering.\n",
    "\n",
    "As we did in the previous case, we used California as testing state, with a downsampled subset of the incidents dataset in order to visualize the results in a clearer way.\n",
    "\n",
    "The code iterates through the most used linkage methods:\n",
    "- **Single Linkage**: Measures the shortest distance between points in two clusters. Sensitive to outliers and tends to form elongated clusters.\n",
    "- **Complete Linkage**: Measures the longest distance between points in two clusters. Tends to produce more compact, spherical clusters.\n",
    "- **Average Linkage**: Uses the average distance between points in two clusters. Balanced approach, less sensitive to outliers.\n",
    "- **Ward Linkage**: Minimizes the variance within clusters. Tends to create equally-sized, compact clusters.\n",
    "\n",
    "Then for each linkage method we visualize the dendrogram and determines the number of clusters formed. Additionally, we calculate the Silhouette Score for each clustering result. This metric measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). Higher Silhouette Scores indicate better-defined clusters.\n",
    "\n",
    "Obtained results regarding the number of clusters generated and the Silhouette Score, have been these:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Linkage Method | Number of Clusters | Silhouette Score |\n",
    "|:--------------:|:------------------:|:----------------:|\n",
    "| Single         | 6                  | 0.198            |\n",
    "| Complete       | 31                 | 0.16             |\n",
    "| Average        | 20                 | 0.133            |\n",
    "| Ward           | 36                 | 0.203            |\n",
    "\n",
    "</center>\n",
    "\n",
    "The number of clusters varies across methods, with Complete and Ward Linkage resulting in higher numbers (31 and 36, respectively) compared to Single and Average Linkage. If the goal is to have a more granular segmentation, Complete or Ward Linkage might be preferred.\n",
    "\n",
    "The Silhouette score provides a measure of how well-defined the clusters are. In this case, Ward Linkage has the highest silhouette score (0.203), suggesting that it produces more distinct clusters. However, the differences in silhouette scores among the methods are relatively small.\n",
    "\n",
    "With respect to the cluster Structure, different linkage methods lead to variations in cluster shapes and sizes. Single Linkage tends to create elongated clusters, while Complete and Ward Linkage aim for more compact clusters. Average Linkage represents a compromise between these extremes.\n",
    "\n",
    "In conclusion, if the goal is to prioritize well-defined, compact clusters, Ward Linkage might be considered the best in this context. However, it's essential to consider the trade-offs between granularity and interpretability when choosing the appropriate hierarchical clustering approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for the state of California\n",
    "california_data = incidents_dataset[incidents_dataset['state'] == 'CALIFORNIA'].copy()\n",
    "\n",
    "# Drop rows with missing values\n",
    "california_data = california_data.dropna()\n",
    "\n",
    "# Select features for clustering\n",
    "selected_features = california_data.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Downsample the dataset for a clearer visualization\n",
    "sampled_data = california_data.sample(frac=0.05, random_state=42)\n",
    "\n",
    "# Standardize features for the downsampled data\n",
    "scaler = StandardScaler()\n",
    "sampled_data_standardized = scaler.fit_transform(sampled_data[selected_features])\n",
    "\n",
    "# Initialize an empty dictionary to store Silhouette Scores for each method\n",
    "silhouette_scores = {}\n",
    "\n",
    "# Choose different linkage methods\n",
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "\n",
    "clusters_hierarchical = []\n",
    "# Compare different clustering results\n",
    "for method in linkage_methods:\n",
    "    # Perform hierarchical clustering on the downsampled data\n",
    "    Z = linkage(sampled_data_standardized, method=method)\n",
    "\n",
    "    # Visualize the dendrogram\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    dendrogram_data = dendrogram(Z, labels=sampled_data.index, leaf_rotation=90, leaf_font_size=8)\n",
    "    plt.title(f'Dendrogram using {method.capitalize()} Linkage (Downsampled)')\n",
    "    plt.xlabel('Data Points')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "\n",
    "    # Cut the dendrogram to obtain clusters\n",
    "    clusters = fcluster(Z, t=5.0, criterion='distance')\n",
    "    clusters_hierarchical.append({\n",
    "                                    'linkage_method': method,\n",
    "                                    'clusters': clusters,\n",
    "                                    'dendrogram': Z\n",
    "                                })\n",
    "    num_clusters = len(np.unique(clusters))\n",
    "    print(f\"Number of Clusters using {method.capitalize()} Linkage (Downsampled): {num_clusters}\")\n",
    "\n",
    "    # Calculate Silhouette Score\n",
    "    silhouette = round(silhouette_score(sampled_data_standardized, clusters), 3)\n",
    "    \n",
    "    # Store Silhouette Score in the dictionary\n",
    "    silhouette_scores[method] = silhouette\n",
    "\n",
    "    # Print the Silhouette Score\n",
    "    print(f\"Silhouette Score using {method.capitalize()} Linkage (Downsampled): {silhouette}\")\n",
    "\n",
    "# Print the dictionary of Silhouette Scores\n",
    "print(\"\\n\\nSilhouette Scores:\", silhouette_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and comparison of the used clustering approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a final evaluation and comparison of the clustering approaches (k-means, density-based clustering, and hierarchical clustering), we will consider several factors:\n",
    "\n",
    "- **Silhouette Score**: Evaluate the silhouette score for each method to assess the quality of the clusters. A higher silhouette score indicates better-defined clusters.\n",
    "- **Interpretability**: Consider the interpretability of the clusters obtained from each method. Are the clusters meaningful and aligned with the characteristics of the data?\n",
    "- **Number of Clusters**: Compare the number of clusters obtained by each method. Consider whether the number of clusters makes sense in the context of our analysis.\n",
    "- **Computational Complexity**: Evaluate the computational complexity of each method, especially if dealing with large datasets. Some methods may be more computationally expensive than others.\n",
    "- **Visualization**: Visualize the clusters obtained from each method using scatter plots, dendrograms, or other relevant visualizations. This can provide insights into the structure of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering results Silhouette Score\n",
    "silhouette_kmeans = round(silhouette_score(data_for_clustering_standardized, clusters_kmeans), 3)\n",
    "print(f\"Silhouette Score (K-means): {silhouette_kmeans}\")\n",
    "\n",
    "# Density-Based Clustering (DBSCAN) results Silhouette Score\n",
    "silhouette_dbscan = round(silhouette_score(california_data_standardized, clusters_dbscan), 3)\n",
    "print(f\"Silhouette Score (DBSCAN): {silhouette_dbscan}\")\n",
    "\n",
    "# Hierarchical Clustering results Silhouette Score\n",
    "for cluster in clusters_hierarchical:\n",
    "    silhouette_hierarchical = silhouette_score(sampled_data_standardized, cluster['clusters'])\n",
    "    print('Silhouette Score (Hierarchical - ' + str(cluster['linkage_method']) + '): ' + str(round(silhouette_hierarchical, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding to the Silhouette score, there is a clear winner. Density based clustering has a high score, compared with the rest of approaches.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Clustering Method          | Silhouette Score |\n",
    "|----------------------------|:----------------:|\n",
    "| K-means                    | 0.168            |\n",
    "| Density-Based              | 0.702            |\n",
    "| (Hierarchical - Single)    | 0.198            |\n",
    "| (Hierarchical - Complete)  | 0.16             |\n",
    "| (Hierarchical - Average)   | 0.133            |\n",
    "| (Hierarchical - Ward)      | 0.203            |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretability \n",
    "K-means assigns each data point to the cluster whose centroid is nearest. Interpretability is relatively high as you can analyze the centroid of each cluster to understand the \"average\" behavior of the points in that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_for_clustering_standardized[:, 0], data_for_clustering_standardized[:, 1], c=clusters_kmeans, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='X', s=200, c='red')\n",
    "plt.title('K-means Clustering with Centroids')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Density-Based Clustering (DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretability\n",
    "\n",
    "DBSCAN identifies clusters based on density-connected regions. The clusters are not required to be of a specific shape, making it suitable for complex data distributions. Interpretability may vary based on the density of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(california_data_standardized[:, 0], california_data_standardized[:, 1], c=clusters_dbscan, cmap='viridis')\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretability\n",
    "Hierarchical clustering organizes data into a tree structure (dendrogram). Interpretability comes from observing the branches and the resulting clusters at different levels of the hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram(cluster['dendrogram'], labels=range(len(sampled_data_standardized)), truncate_mode='lastp', p=12)\n",
    "plt.title(f'Hierarchical Clustering Dendrogram ({cluster[\"linkage_method\"]})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters obtained by each method\n",
    "num_clusters_kmeans = len(set(clusters_kmeans))\n",
    "num_clusters_dbscan = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)  # Exclude noise points for DBSCAN\n",
    "num_clusters_hierarchical = max([max(cluster['clusters']) for cluster in clusters_hierarchical])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of Clusters (K-means): {num_clusters_kmeans}\")\n",
    "print(f\"Number of Clusters (DBSCAN): {num_clusters_dbscan}\")\n",
    "print(f\"Number of Clusters (Hierarchical): {num_clusters_hierarchical}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPHfyRbNzKcv"
   },
   "source": [
    "# Predictive Analysis\n",
    "\n",
    "**Authors:** \n",
    "- Marc Villalonga Llobera\n",
    "- Patxi Juaristi Pagegi\n",
    "\n",
    "**Date:** 08/01/2024\n",
    "\n",
    "---\n",
    "\n",
    "This Jupyter Notebook covers the third task of the project for the Data Mining subject of the Laurea Magistrale of the University of Pisa, focused in predictive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwQ_z05hzsLh"
   },
   "source": [
    "## Environment preparation and data reading\n",
    "\n",
    "First of all, we will install all the required packages, and then import the libraries that we will use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfOUKxJsznAp",
    "outputId": "efba5e14-dcaf-4881-fa82-c4351994e134"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!python -m pip install --upgrade pip\n",
    "#!pip install pandas\n",
    "#!pip install seaborn\n",
    "#!pip install matplotlib\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the required libraries, we will read the datasets that we have exported in the task 1, which they contain the data filtered after the data preparation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three datasets\n",
    "incidents_dataset = pd.read_csv('../project_datasets/incidents_v2.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature Definition\n",
    "\n",
    "First, we will define new feature that will enable classification for later predictions.\n",
    "\n",
    "### Time related features\n",
    "\n",
    "We will extract new features related with the moment that the incident occurred, based on the `date` column.\n",
    "\n",
    "- Extract month, day of the week, and year.\n",
    "- Create a feature for weekends or weekdays.\n",
    "- Create a feature for the season (spring, summer, autumn, winter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format\n",
    "incidents_dataset['date'] = pd.to_datetime(incidents_dataset['date'])\n",
    "\n",
    "# Extract month, day of the week, and year\n",
    "incidents_dataset['month'] = incidents_dataset['date'].dt.month\n",
    "incidents_dataset['day_of_week'] = incidents_dataset['date'].dt.dayofweek\n",
    "incidents_dataset['year'] = incidents_dataset['date'].dt.year\n",
    "\n",
    "# Create a feature for weekends or weekdays\n",
    "incidents_dataset['is_weekend'] = (incidents_dataset['date'].dt.weekday >= 5).astype(int)\n",
    "\n",
    "# Create a feature for the season\n",
    "def get_season(month):\n",
    "    if 3 <= month <= 5:\n",
    "        return 'spring'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'summer'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'autumn'\n",
    "    else:\n",
    "        return 'winter'\n",
    "\n",
    "incidents_dataset['season'] = incidents_dataset['month'].apply(get_season)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(incidents_dataset[['date', 'month', 'day_of_week', 'year', 'is_weekend', 'season']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical and participant features\n",
    "\n",
    "Then, we will create various new features that will take into account the state and city of the incident with the participant features.\n",
    "\n",
    "First of all, we will count the number of incidents per state and per city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City and State Incident Count\n",
    "incidents_dataset['city_incident_count'] = incidents_dataset.groupby('city_or_county')['city_or_county'].transform('count')\n",
    "incidents_dataset['state_incident_count'] = incidents_dataset.groupby('state')['state'].transform('count')\n",
    "\n",
    "print(incidents_dataset[['city_or_county','state', 'city_incident_count', 'state_incident_count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will create two columns, one for the state and the other for the city, where we will define an index, to analyze the severity of the incidents per area. This severity is obtained by the sum of the killed and injured people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City and State Severity Index\n",
    "incidents_dataset['city_severity_index'] = (incidents_dataset['n_killed'] + incidents_dataset['n_injured']) / incidents_dataset['city_incident_count']\n",
    "incidents_dataset['state_severity_index'] = (incidents_dataset['n_killed'] + incidents_dataset['n_injured']) / incidents_dataset['state_incident_count']\n",
    "\n",
    "print(incidents_dataset[['city_or_county','state', 'city_severity_index', 'state_severity_index']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add two columns for analyzing the average age of the incidents per zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City and State Average Age of Participants\n",
    "incidents_dataset['city_avg_age'] = incidents_dataset.groupby('city_or_county')['avg_age_participants'].transform('mean')\n",
    "incidents_dataset['state_avg_age'] = incidents_dataset.groupby('state')['avg_age_participants'].transform('mean')\n",
    "\n",
    "print(incidents_dataset[['city_or_county', 'state', 'city_avg_age', 'state_avg_age']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, since the female participation is quite lower than the male one, we will also add columns to get the female participation in each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City and State Female Participation Rate\n",
    "incidents_dataset['city_female_participation_rate'] = (incidents_dataset['n_females'] / incidents_dataset['n_participants']) * 100\n",
    "incidents_dataset['state_female_participation_rate'] = (incidents_dataset['n_females'] / incidents_dataset.groupby('state')['n_participants'].transform('sum')) * 100\n",
    "\n",
    "print(incidents_dataset[['city_or_county', 'state', 'city_female_participation_rate', 'state_female_participation_rate']][4:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with the preprocessing, we will remove the columns that we will not use in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['date', 'address', 'notes', 'incident_characteristics2', 'latitude', 'longitude',\n",
    "                   'min_age_participants', 'max_age_participants', 'congressional_district', 'state_house_district', 'state_senate_district']\n",
    "incidents_dataset = incidents_dataset.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Categorical Variables\n",
    "\n",
    "Then, we will impute the missing values in numerical columns using the average, and the mode for categorical missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in numerical columns with mean\n",
    "numerical_columns = incidents_dataset.select_dtypes(include=['float64', 'int64']).columns\n",
    "incidents_dataset[numerical_columns] = incidents_dataset[numerical_columns].fillna(incidents_dataset[numerical_columns].mean())\n",
    "\n",
    "# Impute missing values in categorical columns with mode\n",
    "categorical_columns = incidents_dataset.select_dtypes(include=['object']).columns\n",
    "incidents_dataset[categorical_columns] = incidents_dataset[categorical_columns].fillna(incidents_dataset[categorical_columns].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to predict if in the incident there have been at least a killed person or not, we will create a binary variable, which will say whether there have been deaths or not in the incidents of the dataset. This will be obtained from the variable `n_killed`, if it is greater than 0 it will be *True*, and if it is not, it will be *False*. The name of the variable will be `people_killed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary target variable 'has_fatality'\n",
    "incidents_dataset['people_killed'] = (incidents_dataset['n_killed'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by specifying a dictionary `categorical_columns_to_encode` that indicates the categorical columns to be one-hot encoded along with their respective thresholds. We have set the thresholds to control the number of unique values retained for each categorical column during one-hot encoding and limit the categories just to the most relevant values, because including all values, makes the dataset too big, and we experienced memory problems during the analysis.\n",
    "\n",
    "Moreover, analyzing incidents with very rare characteristics does not make sense. Anyway, columns like state, gender, age group or season do not have any threshold, because we want to include all of them. For example, age groups are just three, genders are two, and seasons are four, so it is unnecessary to limit the distinct values.\n",
    "\n",
    "Afterwards, we go through the list of categorical columns and perform the one-hot encoding using `get_dummies` function. For each categorical column specified in the dictionary, we either include all unique values or select the top values based on the provided threshold. Any values not meeting the threshold are grouped into an \"Other\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns for one-hot encoding along with their respective thresholds\n",
    "categorical_columns_to_encode = {\n",
    "    'state': None,  # Set to None to include all distinct values\n",
    "    'city_or_county': 100,\n",
    "    'participant_gender1': None,\n",
    "    'participant_age_group1': None,\n",
    "    'incident_characteristics1': 20,\n",
    "    'season': None\n",
    "}\n",
    "\n",
    "# Perform one-hot encoding for categorical variables. Use sparse representation for one-hot encoding\n",
    "for column, threshold in categorical_columns_to_encode.items():\n",
    "    if threshold is None:\n",
    "        top_values = incidents_dataset[column].unique()\n",
    "    else:\n",
    "        top_values = incidents_dataset[column].value_counts().nlargest(threshold).index\n",
    "    incidents_dataset[column] = incidents_dataset[column].where(incidents_dataset[column].isin(top_values), 'Other')\n",
    "\n",
    "incidents_dataset = pd.get_dummies(incidents_dataset, columns=categorical_columns_to_encode.keys(), sparse=True)\n",
    "\n",
    "print(incidents_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target variable 'y'\n",
    "y = incidents_dataset['people_killed']\n",
    "\n",
    "# Extract features 'X'\n",
    "X = incidents_dataset.drop(columns=['n_killed', 'people_killed'])\n",
    "\n",
    "# Convert the dataframe to a dense array\n",
    "X_array = X.to_numpy()\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_array)\n",
    "\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "# Check the shape of the resulting sets\n",
    "print(\"\\nTrain set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next section will focus on the model selection and evaluation. For that, we perform a classification task using five different classifiers: K-Nearest Neighbors (KNN), Naive Bayes, Support Vector Machines (SVM), Logistic Regression and Random Forest. Each classifier is instantiated and stored in a dictionary named classifiers. The LogisticRegression model is configured with a higher maximum number of iterations (max_iter=1000) to address potential convergence issues.\n",
    "\n",
    "We iterate over each classifier, fitting the model to the training data (`X_train`, `y_train`) and making predictions on the test set (`X_test`). After predicting, we calculate various performance metrics, including accuracy, precision, recall, F1 score, and area under the ROC curve (AUC-ROC). These metrics are essential for evaluating how well each classifier performs on the test set.\n",
    "\n",
    "Additionally, the code employs cross-validation (`cross_val_score`) with a 3-fold validation to assess the models robustly. Although this technique is time-consuming to obtain the results, it helps to estimate the performance on different subsets of the data, providing insights into its generalization ability.\n",
    "\n",
    "The results, including individual metric scores, cross-validation AUC-ROC scores, and the mean cross-validation AUC-ROC, are printed for each classifier. This comprehensive evaluation allows for a comparison of the classifiers' performance and helps in selecting the most suitable model for the given classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Support Vector Machines': SVC(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Create lists to store metrics for plotting\n",
    "classifier_names = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "cv_mean_scores = []\n",
    "\n",
    "# Iterate over classifiers\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # Cross-validation for more robust performance assessment\n",
    "    cv_scores = cross_val_score(clf, X_scaled, y, cv=3, scoring='roc_auc')\n",
    "    cv_mean = cv_scores.mean()\n",
    "\n",
    "    # Append metrics to lists\n",
    "    classifier_names.append(clf_name)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    cv_mean_scores.append(cv_mean)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nClassifier: {clf_name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
    "    print(f\"Cross-Validation AUC-ROC Scores: {cv_scores}\")\n",
    "    print(f\"Mean Cross-Validation AUC-ROC: {cv_scores.mean()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see clearly the difference between the classifiers, we have made a plot which includes six subplots with the evaluation metrics that we tested before. Moreover, we have included an arrow for the classifier that has obtained the best result in each evaluation metric.\n",
    "\n",
    "Analyzing the performance of various classifiers, it's evident that each model excels in different aspects. Logistic Regression and Random Forest showcase high precision, making them suitable for tasks where minimizing false positives is crucial. Naive Bayes, on the other hand, exhibit higher recall, making it favorable when capturing as many positive instances as possible is a priority. Support Vector Machines (SVM) and Logistic Regression stand out for their balanced performance across multiple metrics, making them strong contenders for scenarios where a harmonious blend of precision and recall is desired. By contrast, compared to the other classifiers, K-Nearest Neighbors obtained the worst results in all metrics.\n",
    "\n",
    "Analyzing the set of results obtained in the different metrics, it can be said that the most complete classifier for this case is the Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to round scores and create legend labels\n",
    "def round_and_label(scores):\n",
    "    rounded_scores = np.round(scores, 4)\n",
    "    return [f'{score:.4f}' for score in rounded_scores]\n",
    "\n",
    "# Plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 13))\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.barplot(x=classifier_names, y=accuracy_scores, palette=\"magma\", hue=round_and_label(accuracy_scores))\n",
    "max_index = np.argmax(accuracy_scores)\n",
    "plt.annotate('Max', xy=(max_index, accuracy_scores[max_index]), xytext=(max_index, max(accuracy_scores) + 0.1),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=8, ha='center')\n",
    "plt.title(\"Accuracy Scores\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1, 0))\n",
    "\n",
    "# Precision Plot\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.barplot(x=classifier_names, y=precision_scores, palette=\"magma\", hue=round_and_label(precision_scores))\n",
    "max_index = np.argmax(precision_scores)\n",
    "plt.annotate('Max', xy=(max_index, precision_scores[max_index]), xytext=(max_index, max(precision_scores) + 0.1),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=8, ha='center')\n",
    "plt.title(\"Precision Scores\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1, 0))\n",
    "\n",
    "# Recall Plot\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.barplot(x=classifier_names, y=recall_scores, palette=\"magma\", hue=round_and_label(recall_scores))\n",
    "max_index = np.argmax(recall_scores)\n",
    "plt.annotate('Max', xy=(max_index, recall_scores[max_index]), xytext=(max_index, max(recall_scores) + 0.1),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=8, ha='center')\n",
    "plt.title(\"Recall Scores\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1, 0))\n",
    "\n",
    "# F1 Score Plot\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.barplot(x=classifier_names, y=f1_scores, palette=\"magma\", hue=round_and_label(f1_scores))\n",
    "max_index = np.argmax(f1_scores)\n",
    "plt.annotate('Max', xy=(max_index, f1_scores[max_index]), xytext=(max_index, max(f1_scores) + 0.1),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=8, ha='center')\n",
    "plt.title(\"F1 Scores\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1, 0))\n",
    "\n",
    "# ROC-AUC Plot\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.barplot(x=classifier_names, y=roc_auc_scores, palette=\"magma\", hue=round_and_label(roc_auc_scores))\n",
    "max_index = np.argmax(roc_auc_scores)\n",
    "plt.annotate('Max', xy=(max_index, roc_auc_scores[max_index]), xytext=(max_index, max(roc_auc_scores) + 0.1),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=8, ha='center')\n",
    "plt.title(\"ROC-AUC Scores\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1, 0))\n",
    "\n",
    "# Cross-Validation Mean ROC-AUC Plot\n",
    "plt.subplot(2, 3, 6)\n",
    "sns.barplot(x=classifier_names, y=cv_mean_scores, palette=\"magma\", hue=round_and_label(cv_mean_scores))\n",
    "max_index = np.argmax(cv_mean_scores)\n",
    "plt.annotate('Max', xy=(max_index, cv_mean_scores[max_index]), xytext=(max_index, max(cv_mean_scores) + 0.1),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05),\n",
    "             fontsize=8, ha='center')\n",
    "plt.title(\"Mean Cross-Validation ROC-AUC Scores\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='lower right', bbox_to_anchor=(1, 0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

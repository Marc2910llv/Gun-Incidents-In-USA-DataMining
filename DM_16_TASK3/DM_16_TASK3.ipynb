{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPHfyRbNzKcv"
   },
   "source": [
    "# Predictive Analysis\n",
    "\n",
    "**Authors:** \n",
    "- Marc Villalonga Llobera\n",
    "- Patxi Juaristi Pagegi\n",
    "\n",
    "**Date:** 08/01/2024\n",
    "\n",
    "---\n",
    "\n",
    "This Jupyter Notebook covers the third task of the project for the Data Mining subject of the Laurea Magistrale of the University of Pisa, focused in predictive analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwQ_z05hzsLh"
   },
   "source": [
    "## Environment preparation and data reading\n",
    "\n",
    "First of all, we will install all the required packages, and then import the libraries that we will use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfOUKxJsznAp",
    "outputId": "efba5e14-dcaf-4881-fa82-c4351994e134"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!python -m pip install --upgrade pip\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "\n",
    "# AÃ±adir aqui si necesitamos otras librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the required libraries, we will read the datasets that we have exported in the task 1, which they contain the data filtered after the data preparation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three datasets\n",
    "incidents_dataset = pd.read_csv('../project_datasets/incidents_v2.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature Definition\n",
    "\n",
    "First, we will define new feature that will enable classification for later predictions.\n",
    "\n",
    "### Time related features\n",
    "\n",
    "We will extract new features related with the moment that the incident occurred, based on the `date` column.\n",
    "\n",
    "- Extract month, day of the week, and year.\n",
    "- Create a feature for weekends or weekdays.\n",
    "- Create a feature for the season (spring, summer, autumn, winter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format\n",
    "incidents_dataset['date'] = pd.to_datetime(incidents_dataset['date'])\n",
    "\n",
    "# Extract month, day of the week, and year\n",
    "incidents_dataset['month'] = incidents_dataset['date'].dt.month\n",
    "incidents_dataset['day_of_week'] = incidents_dataset['date'].dt.dayofweek\n",
    "incidents_dataset['year'] = incidents_dataset['date'].dt.year\n",
    "\n",
    "# Create a feature for weekends or weekdays\n",
    "incidents_dataset['is_weekend'] = (incidents_dataset['date'].dt.weekday >= 5).astype(int)\n",
    "\n",
    "# Create a feature for the season\n",
    "def get_season(month):\n",
    "    if 3 <= month <= 5:\n",
    "        return 'spring'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'summer'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'autumn'\n",
    "    else:\n",
    "        return 'winter'\n",
    "\n",
    "incidents_dataset['season'] = incidents_dataset['month'].apply(get_season)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(incidents_dataset[['date', 'month', 'day_of_week', 'year', 'is_weekend', 'season']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical and participant features\n",
    "\n",
    "Then, we will create various new features that will take into account the state and city of the incident with the participant features.\n",
    "\n",
    "First of all, we will count the number of incidents per state and per city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City and State Incident Count\n",
    "incidents_dataset['city_incident_count'] = incidents_dataset.groupby('city_or_county')['city_or_county'].transform('count')\n",
    "incidents_dataset['state_incident_count'] = incidents_dataset.groupby('state')['state'].transform('count')\n",
    "\n",
    "print(incidents_dataset[['city_or_county','state', 'city_incident_count', 'state_incident_count']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will create two columns, one for the state and the other for the city, where we will define an index, to analyze the severity of the incidents per area. This severity is obtained by the sum of the killed and injured people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City and State Severity Index\n",
    "incidents_dataset['city_severity_index'] = (incidents_dataset['n_killed'] + incidents_dataset['n_injured']) / incidents_dataset['city_incident_count']\n",
    "incidents_dataset['state_severity_index'] = (incidents_dataset['n_killed'] + incidents_dataset['n_injured']) / incidents_dataset['state_incident_count']\n",
    "\n",
    "print(incidents_dataset[['city_or_county','state', 'city_severity_index', 'state_severity_index']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also add two columns for analyzing the average age of the incidents per zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City and State Average Age of Participants\n",
    "incidents_dataset['city_avg_age'] = incidents_dataset.groupby('city_or_county')['avg_age_participants'].transform('mean')\n",
    "incidents_dataset['state_avg_age'] = incidents_dataset.groupby('state')['avg_age_participants'].transform('mean')\n",
    "\n",
    "print(incidents_dataset[['city_or_county', 'state', 'city_avg_age', 'state_avg_age']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, since the female participation is quite lower than the male one, we will also add columns to get the female participation in each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City and State Female Participation Rate\n",
    "incidents_dataset['city_female_participation_rate'] = (incidents_dataset['n_females'] / incidents_dataset['n_participants']) * 100\n",
    "incidents_dataset['state_female_participation_rate'] = (incidents_dataset['n_females'] / incidents_dataset.groupby('state')['n_participants'].transform('sum')) * 100\n",
    "\n",
    "print(incidents_dataset[['city_or_county', 'state', 'city_female_participation_rate', 'state_female_participation_rate']][4:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with the preprocessing, we will remove the columns that we will not use in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['date', 'address', 'notes', 'incident_characteristics2', 'latitude', 'longitude',\n",
    "                   'min_age_participants', 'max_age_participants', 'congressional_district', 'state_house_district', 'state_senate_district']\n",
    "incidents_dataset = incidents_dataset.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Categorical Variables\n",
    "\n",
    "Then, we will impute the missing values in numerical columns using the average, and the mode for categorical missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in numerical columns with mean\n",
    "numerical_columns = incidents_dataset.select_dtypes(include=['float64', 'int64']).columns\n",
    "incidents_dataset[numerical_columns] = incidents_dataset[numerical_columns].fillna(incidents_dataset[numerical_columns].mean())\n",
    "\n",
    "# Impute missing values in categorical columns with mode\n",
    "categorical_columns = incidents_dataset.select_dtypes(include=['object']).columns\n",
    "incidents_dataset[categorical_columns] = incidents_dataset[categorical_columns].fillna(incidents_dataset[categorical_columns].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are going to predict if in the incident there have been at least a killed person or not, we will create a binary variable, which will say whether there have been deaths or not in the incidents of the dataset. This will be obtained from the variable `n_killed`, if it is greater than 0 it will be *True*, and if it is not, it will be *False*. The name of the variable will be `people_killed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary target variable 'has_fatality'\n",
    "incidents_dataset['people_killed'] = (incidents_dataset['n_killed'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by specifying a dictionary `categorical_columns_to_encode` that indicates the categorical columns to be one-hot encoded along with their respective thresholds. We have set the thresholds to control the number of unique values retained for each categorical column during one-hot encoding and limit the categories just to the most relevant values, because including all values, makes the dataset too big, and we experienced memory problems during the analysis.\n",
    "\n",
    "Moreover, analyzing incidents with very rare characteristics does not make sense. Anyway, columns like state, gender, age group or season do not have any threshold, because we want to include all of them. For example, age groups are just three, genders are two, and seasons are four, so it is unnecessary to limit the distinct values.\n",
    "\n",
    "Afterwards, we go through the list of categorical columns and perform the one-hot encoding using `get_dummies` function. For each categorical column specified in the dictionary, we either include all unique values or select the top values based on the provided threshold. Any values not meeting the threshold are grouped into an \"Other\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns for one-hot encoding along with their respective thresholds\n",
    "categorical_columns_to_encode = {\n",
    "    'state': None,  # Set to None to include all distinct values\n",
    "    'city_or_county': 100,\n",
    "    'participant_gender1': None,\n",
    "    'participant_age_group1': None,\n",
    "    'incident_characteristics1': 20,\n",
    "    'season': None\n",
    "}\n",
    "\n",
    "# Perform one-hot encoding for categorical variables. Use sparse representation for one-hot encoding\n",
    "for column, threshold in categorical_columns_to_encode.items():\n",
    "    if threshold is None:\n",
    "        top_values = incidents_dataset[column].unique()\n",
    "    else:\n",
    "        top_values = incidents_dataset[column].value_counts().nlargest(threshold).index\n",
    "    incidents_dataset[column] = incidents_dataset[column].where(incidents_dataset[column].isin(top_values), 'Other')\n",
    "\n",
    "incidents_dataset = pd.get_dummies(incidents_dataset, columns=categorical_columns_to_encode.keys(), sparse=True)\n",
    "\n",
    "print(incidents_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract target variable 'y'\n",
    "y = incidents_dataset['people_killed']\n",
    "\n",
    "# Extract features 'X'\n",
    "X = incidents_dataset.drop(columns=['n_killed', 'people_killed'])\n",
    "\n",
    "# Convert the dataframe to a dense array\n",
    "X_array = X.to_numpy()\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_array)\n",
    "\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "# Check the shape of the resulting sets\n",
    "print(\"\\nTrain set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Evaluation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPHfyRbNzKcv"
   },
   "source": [
    "# Data Analysis Jupyter Notebook\n",
    "\n",
    "This Jupyter Notebook is a template for data analysis. It includes installation of necessary libraries, loading datasets, and displaying basic information and statistics for each dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwQ_z05hzsLh"
   },
   "source": [
    "## Environment preparation\n",
    "\n",
    "First of all, we will install all the required packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfOUKxJsznAp",
    "outputId": "efba5e14-dcaf-4881-fa82-c4351994e134"
   },
   "outputs": [],
   "source": [
    "#capture\n",
    "#!python -m pip install --upgrade pip\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "#!pip install seaborn\n",
    "#!pip install scipy\n",
    "#!pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it-bBY0rz4c9"
   },
   "source": [
    "## Data Understanding and preparation\n",
    "\n",
    "We will read the datasets in order to understand the information that they contain, and adapt them to the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8h4TwXZMx9KN",
    "outputId": "b53599ac-a442-40c2-890a-c2af5e0e18d3"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the three datasets\n",
    "incidents_dataset = pd.read_csv('./project_datasets/incidents.csv', low_memory=False)\n",
    "pbsy_dataset = pd.read_csv('./project_datasets/povertyByStateYear.csv')\n",
    "ysdh_dataset = pd.read_csv('./project_datasets/year_state_district_house.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the datasets, we will execute some commands to have a preview about the information.\n",
    "\n",
    "With `info()`, we will see the names of the columns of the datasets, the data type and the amount of non null values that they have. With `head()`, the first 5 rows of each dataset, that they will help us to have the idea of the dataset row types. Eventually, with `describe()` we will obtain the statisctical values of the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0VFQypQ2fRn",
    "outputId": "d2bf1ede-9e41-4f94-9abb-c5500255a1ce"
   },
   "outputs": [],
   "source": [
    "# Display basic information about each dataset\n",
    "print(\"\\n------- Incidents Info:-------\")\n",
    "print(incidents_dataset.info())\n",
    "\n",
    "print(\"\\n------- Dataset 2 Info:-------\")\n",
    "print(pbsy_dataset.info())\n",
    "\n",
    "print(\"\\n------- Dataset 3 Info:-------\")\n",
    "print(ysdh_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5519wFl2di4",
    "outputId": "32dde122-4939-4201-dda2-7838f0e97d93"
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of each dataset\n",
    "print(\"------- Incidents -------\")\n",
    "print(incidents_dataset.head())\n",
    "\n",
    "print(\"\\n------- Poverty By State and Year -------\")\n",
    "print(pbsy_dataset.head())\n",
    "\n",
    "print(\"\\n------- Year State District House -------\")\n",
    "print(ysdh_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the data types of some of the columns are not correct, so we will fix them. For example, `date` columns has data type `object`, so we will convert it on datetime and some columns that contain numeric values are as `object` type as well, so we will change them. Also here, the modifications are done in the incidents dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime\n",
    "incidents_dataset['date'] = pd.to_datetime(incidents_dataset['date'])\n",
    "\n",
    "# Convert numeric data from object to numeric, handling errors\n",
    "incidents_dataset['state_senate_district'] = pd.to_numeric(incidents_dataset['state_senate_district'], errors='coerce')\n",
    "incidents_dataset['min_age_participants'] = pd.to_numeric(incidents_dataset['min_age_participants'], errors='coerce')\n",
    "incidents_dataset['max_age_participants'] = pd.to_numeric(incidents_dataset['max_age_participants'], errors='coerce')\n",
    "incidents_dataset['avg_age_participants'] = pd.to_numeric(incidents_dataset['avg_age_participants'], errors='coerce')\n",
    "incidents_dataset['n_participants_child'] = pd.to_numeric(incidents_dataset['n_participants_child'], errors='coerce').astype('Int64')\n",
    "incidents_dataset['n_participants_teen'] = pd.to_numeric(incidents_dataset['n_participants_teen'], errors='coerce').astype('Int64')\n",
    "incidents_dataset['n_participants_adult'] = pd.to_numeric(incidents_dataset['n_participants_adult'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Verify the data types after conversion\n",
    "print(\"\\nData Types After Conversion:\\n ---- Dataset 1 ----\\n\", incidents_dataset.dtypes)\n",
    "print(\"\\n ---- Dataset 2 ----\\n\", pbsy_dataset.dtypes)\n",
    "print(\"\\n ---- Dataset 3 ----\\n\", ysdh_dataset.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1HrThg42fZE",
    "outputId": "a2868822-604c-410b-a563-9b6a6331906d"
   },
   "outputs": [],
   "source": [
    "# Display basic statistics for each dataset\n",
    "print(\"\\n------- Incidents Statistics:-------\")\n",
    "print(incidents_dataset.describe())\n",
    "\n",
    "print(\"\\n------- Poverty By State and Year Statistics:-------\")\n",
    "print(pbsy_dataset.describe())\n",
    "\n",
    "print(\"\\n------- Dataset 3 Statistics:-------\")\n",
    "print(ysdh_dataset.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets columns description\n",
    "\n",
    "After analyzing the datasets with the previous commands, we will briefly describe each of the columns of the three datasets:\n",
    "\n",
    "#### Incidents dataset (incidents.csv)\n",
    "\n",
    "- **Date of Incident (`date`):** This variable represents the date when the gun incident occurred.\n",
    "- **State (`state`):** Indicates the state where the incident took place.\n",
    "- **City or County (`city_or_county`):** Specifies the city or county where the incident occurred.\n",
    "- **Address (`address`):** Represents the specific address where the incident took place.\n",
    "- **Geographical Coordinates (`latitude, longitude`):** Provides the latitude and longitude of the incident location.\n",
    "- **Congressional District (`congressional_district`):** Specifies the congressional district where the incident occurred.\n",
    "- **State House District (`state_house_district`):** Represents the state house district of the incident.\n",
    "- **State Senate District (`state_senate_district`):** Indicates the state senate district where the incident took place.\n",
    "- **Participant Age (`participant_age1`):** Represents the exact age of one randomly chosen participant in the incident.\n",
    "- **Participant Age Group (`participant_age_group1`):** Specifies the age group of one randomly chosen participant.\n",
    "- **Participant Gender (`participant_gender1`):** Indicates the gender of one randomly chosen participant.\n",
    "- **Minimum, Average, and Maximum Age of Participants (`min_age_participants, avg_age_participants, max_age_participants`):** Provide statistical measures of participant ages.\n",
    "- **Number of Participants by Age Group (`n_participants_child, n_participants_teen, n_participants_adult`):** Gives the count of participants in different age groups.\n",
    "- **Number of Males and Females (`n_males, n_females`):** Specifies the count of male and female participants.\n",
    "- **Number of People Killed and Injured (`n_killed, n_injured`):** Represents the count of people killed and injured in the incident.\n",
    "- **Number of Arrested and Unharmed Participants (`n_arrested, n_unharmed`):** Indicates the count of participants arrested and unharmed.\n",
    "- **Total Number of Participants (`n_participants`):** Represents the total number of participants in the incident.\n",
    "- **Additional Notes (`notes`):** Provides additional information or notes about the incident.\n",
    "- **Incident Characteristics (`incident_characteristics1, incident_characteristics2`):** Specifies the characteristics of the incident.\n",
    "\n",
    "#### Poverty by state and year dataset (povertyByStateYear.csv)\n",
    "\n",
    "- **State (`state`):** The name of the state.\n",
    "- **Year (`year`):** The year for which the poverty rate is recorded.\n",
    "- **Poverty percentage (`povertyPercentage`):** The percentage of the population in poverty for a specific state and year.\n",
    "\n",
    "#### Year state district house dataset (year_state_district_house.csv)\n",
    "\n",
    "- **Year (`year`):** The year of the election.\n",
    "- **State (`state`):** The name of the state for which election results are recorded.\n",
    "- **State (`congressional_district`):** The congressional district number for which election results are recorded.\n",
    "- **Party (`party`):** The political party associated with the candidate.\n",
    "- **Candidate votes (`candidatevotes`):** The number of votes received by a specific candidate in a particular congressional district.\n",
    "- **Total votes (`totalvotes`):** The total number of votes cast in a particular congressional district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMe9jOrW5aWI"
   },
   "source": [
    "### Data quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having the idea of the content of the dataset, we will perform an analysis of the quality of the data.\n",
    "\n",
    "Firstly, we will check for missing values in the dataset 1 columns. We will perform this analysis in the dataset of incidents only, because the other two do not have missing values.\n",
    "\n",
    "With the next graph we can see the distribution of missing values for each column. For example, we can see that the column with more missing values is  `incident_characteristic2`. This occurs because most of the incidents do not have two characteristics, but just one, since we can see that `incident_characteristic1` has almost no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "id": "Jh8ZPjQp5HsW",
    "outputId": "ca13b341-6656-4303-ca0f-d4da93167d6c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = incidents_dataset.isnull().sum()\n",
    "#print(\"\\nMissing Values:\\n\", missing_values)\n",
    "\n",
    "# Plotting the sum of missing values for each column using a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=missing_values.index, y=missing_values.values, hue=missing_values.index, palette='flare', legend=False)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Sum of Missing Values in Incidents Dataset')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases that do not have the characteristic specified (`incident_characteristics1` = *null*), would be removed, since we would not know what was the incident about, and it does not make sense to analyze them.\n",
    "\n",
    "The data that doesn't have any `address` and any `latitude` and `longitude` will be eliminated since it does not make sense that an incident has not occurred anywhere and also the number of incidents with this condition is insignificant compared to the number of total incidents. The cases that have an address but no coordinates, or vice versa, would be maintained.\n",
    "\n",
    "With the column of `participant_age1` we will fill the nulls with the average age of the participants, if this is not null.\n",
    "\n",
    "Although many other columns have empty values, are columns that null values are possible, since they do not necessarily have to have values. Therefore, we will leave them as there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Number of Rows in the Original Dataset:\", incidents_dataset.shape[0])\n",
    "\n",
    "# Eliminate the data that have null in incident_characteristics1, adress, latitude and adress.\n",
    "incidents_dataset = incidents_dataset.dropna(subset=['incident_characteristics1'], how='all')\n",
    "incidents_dataset = incidents_dataset.dropna(subset=['latitude', 'longitude', 'address'], how='all')\n",
    "\n",
    "# Rename the other nulls with no_data\n",
    "valor_reemplazo = 'no_data'\n",
    "actualGender = 'Male'\n",
    "totalChilds = 0\n",
    "totalTeens = 0\n",
    "totalAdults = 0\n",
    "totalMales = 0\n",
    "totalFemales = 0\n",
    "totalArrested = 0\n",
    "totalUnharmed = 0 \n",
    "def setAgeGroup(a):\n",
    "    if a < 12:\n",
    "        return 'Child 0-11'\n",
    "    elif a < 18:\n",
    "        return 'Teen 12-17'\n",
    "    return 'Adult 18+'\n",
    "def sumTotalNumber(a, t):\n",
    "    if a is not None:\n",
    "        return t\n",
    "    else:\n",
    "        return t + a\n",
    "\n",
    "actualGender = 'Male'  # Asegúrate de inicializar actualGender antes del bucle si aún no está definido\n",
    "totalChilds, totalTeens, totalAdults, totalMales, totalFemales, totalArrested, totalUnharmed = 0, 0, 0, 0, 0, 0, 0  # Inicializar las variables acumulativas\n",
    "\n",
    "for index, x in incidents_dataset.iterrows():\n",
    "    # Primero limpiamos las edades mínima, promedio y máxima de los participantes\n",
    "    if pd.isnull(x['min_age_participants']):\n",
    "        if not pd.isnull(x['avg_age_participants']):\n",
    "            if not pd.isnull(x['max_age_participants']):\n",
    "                k = x['avg_age_participants'] - (x['max_age_participants'] - x['avg_age_participants'])\n",
    "                if k < 0:\n",
    "                    incidents_dataset.at[index, 'min_age_participants'] = 0\n",
    "                else:\n",
    "                    incidents_dataset.at[index, 'min_age_participants'] = k\n",
    "            else:\n",
    "                incidents_dataset.at[index, 'min_age_participants'] = x['avg_age_participants']\n",
    "\n",
    "    if pd.isnull(x['max_age_participants']):\n",
    "        if not pd.isnull(x['avg_age_participants']):\n",
    "            if not pd.isnull(x['min_age_participants']):\n",
    "                incidents_dataset.at[index, 'max_age_participants'] = x['avg_age_participants'] + (x['avg_age_participants'] - x['min_age_participants'])\n",
    "            else:\n",
    "                incidents_dataset.at[index, 'max_age_participants'] = x['avg_age_participants']\n",
    "\n",
    "    if pd.isnull(x['avg_age_participants']):\n",
    "        if not (pd.isnull(x['min_age_participants']) or pd.isnull(x['max_age_participants'])):\n",
    "            incidents_dataset.at[index, 'avg_age_participants'] = (float(x['min_age_participants']) + float(x['max_age_participants'])) / 2\n",
    "        elif pd.isnull(x['min_age_participants']):\n",
    "            incidents_dataset.at[index, 'avg_age_participants'] = float(x['max_age_participants'])\n",
    "        else:\n",
    "            incidents_dataset.at[index, 'avg_age_participants'] = float(x['min_age_participants'])\n",
    "\n",
    "    # Para la edad aleatoria del participante, usaremos el promedio; si es nula, usamos la mínima o la máxima\n",
    "    if pd.isnull(x['participant_age1']):\n",
    "        if (not pd.isnull(x['avg_age_participants'])) and (x['avg_age_participants'] > 0):\n",
    "            incidents_dataset.at[index, 'participant_age1'] = float(x['avg_age_participants'])\n",
    "        elif (not pd.isnull(x['min_age_participants'])) and (x['min_age_participants'] > 0):\n",
    "            incidents_dataset.at[index, 'participant_age1'] = float(x['min_age_participants'])\n",
    "        elif (not pd.isnull(x['max_age_participants'])) and (x['max_age_participants'] > 0): \n",
    "            incidents_dataset.at[index, 'participant_age1'] = float(x['max_age_participants'])\n",
    "\n",
    "    # Para el grupo de edad aleatorio, hacemos lo mismo que para participant_age1\n",
    "    if pd.isnull(x['participant_age_group1']):\n",
    "        if not pd.isnull(x['avg_age_participants']):\n",
    "            incidents_dataset.at[index, 'participant_age_group1'] = setAgeGroup(float(x['avg_age_participants']))\n",
    "        elif not pd.isnull(x['min_age_participants']):\n",
    "            incidents_dataset.at[index, 'participant_age_group1'] = setAgeGroup(float(x['min_age_participants']))\n",
    "        elif not pd.isnull(x['max_age_participants']):\n",
    "            incidents_dataset.at[index, 'participant_age_group1'] = setAgeGroup(float(x['max_age_participants']))\n",
    "        elif not pd.isnull(x['participant_age1']):\n",
    "            incidents_dataset.at[index, 'participant_age_group1'] = setAgeGroup(float(x['participant_age1']))\n",
    "\n",
    "    # Para la edad aleatoria del participante de cada fila, la llenamos con un género diferente al anterior\n",
    "    if pd.isnull(x['participant_gender1']):\n",
    "        incidents_dataset.at[index, 'participant_gender1'] = actualGender\n",
    "        actualGender = 'Female' if actualGender == 'Male' else 'Male'\n",
    "\n",
    "    # Acumular totales\n",
    "    totalChilds = sumTotalNumber(x['n_participants_child'], totalChilds)\n",
    "    totalTeens = sumTotalNumber(x['n_participants_teen'], totalTeens)\n",
    "    totalAdults = sumTotalNumber(x['n_participants_adult'], totalAdults)\n",
    "    totalMales = sumTotalNumber(x['n_males'], totalMales)\n",
    "    totalFemales = sumTotalNumber(x['n_females'], totalFemales)\n",
    "    totalArrested = sumTotalNumber(x['n_arrested'], totalArrested)\n",
    "    totalUnharmed = sumTotalNumber(x['n_unharmed'], totalUnharmed)\n",
    "\n",
    "#For the rest numeral attributes we just do the average of all the cases\n",
    "incidents_dataset['n_participants_child'].fillna(round(totalChilds/len(incidents_dataset)), inplace=True)\n",
    "incidents_dataset['n_participants_teen'].fillna(round(totalTeens/len(incidents_dataset)), inplace=True)\n",
    "incidents_dataset['n_participants_adult'].fillna(round(totalAdults/len(incidents_dataset)), inplace=True)\n",
    "incidents_dataset['n_males'].fillna(round(totalMales/len(incidents_dataset)), inplace=True)\n",
    "incidents_dataset['n_females'].fillna(round(totalFemales/len(incidents_dataset)), inplace=True)\n",
    "incidents_dataset['n_arrested'].fillna(round(totalArrested/len(incidents_dataset)), inplace=True)\n",
    "incidents_dataset['n_unharmed'].fillna(round(totalUnharmed/len(incidents_dataset)), inplace=True)\n",
    "\n",
    "# Print mising values label\n",
    "missing_values = incidents_dataset.isnull().sum()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=missing_values.index, y=missing_values.values, hue=missing_values.index, palette='flare', legend=False)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Sum of Missing Values in Incidents Dataset')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will check for duplicated values in the datasets. We can see that the incidents dataset is the only one that has duplicates, so we remove them and then verify that we have done it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rA-N7o_O5l2Z",
    "outputId": "29b40d78-96b9-449b-dde1-ff13f93ef20a"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "d1_duplicates = incidents_dataset.duplicated().sum()\n",
    "print(\"- Number of Duplicates:\", d1_duplicates)\n",
    "\n",
    "# Remove duplicate rows\n",
    "incidents_dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "# Verify the removal of duplicates\n",
    "d1_duplicates_after_removal = incidents_dataset.duplicated().sum()\n",
    "print(\"\\n- Number of Duplicates After Removal:\", d1_duplicates_after_removal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1BrLIG4IlsT",
    "outputId": "a1c8227d-bcef-4f1d-d1ea-1157235574ad"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "d2_duplicates = pbsy_dataset.duplicated().sum()\n",
    "print(\"- Number of Duplicates:\", d2_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5sVyh1FIlv8",
    "outputId": "b83a2e16-21c9-4fed-adc9-7379d5b2e2f9"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "d3_duplicates = ysdh_dataset.duplicated().sum()\n",
    "print(\"- Number of Duplicates:\", d3_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, and before starting with further modifications, we will establish the same capitalization for the state column, because the elections dataset has the state in upper case, while the other two not. This will help with further merge and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "incidents_dataset['state'] = incidents_dataset['state'].str.upper()\n",
    "pbsy_dataset['state'] = pbsy_dataset['state'].str.upper()\n",
    "ysdh_dataset['state'] = ysdh_dataset['state'].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmhv3ksq7Y-r"
   },
   "source": [
    "### Distribution of variables\n",
    "\n",
    "Then, we will see the distribution of the variables in the different datasest.\n",
    "\n",
    "#### Poverty percentages evaluation\n",
    "\n",
    "To start, we will see the distribution of the poverty rates by states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=pbsy_dataset, x='state', y='povertyPercentage', hue='state', palette='viridis', dodge=False)\n",
    "plt.title('Poverty Percentage by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Poverty Percentage')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chart, we can see that the first state is \"United States\". This is not correct, because the United States is the country, but not a state. Even tough we will not remove or modify it, we have to take it into account when doing analysis. In the cases that we analyze global poverty rates among the country, there is no problem using this dataset, but if we want to distinguish among states, or make analysis where the states have relevance, we should remove the rows that have the column state as \"United States\". After all the modifications we will create two different datasets for each case.\n",
    "\n",
    "Then, we will make a boxplot so see the evolution of the poverty over the years, as well as the distribution of the poverty ranges in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=pbsy_dataset, x='year', y='povertyPercentage', hue='year', palette='viridis', dodge=False)\n",
    "plt.title('Poverty Percentage Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Poverty Percentage')\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the previous graph that there are no values in the year 2012. To check that we can filter the data by year and not *null* data, and check that there are no rows. Nevertheless, there are 52 rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_2012 = pbsy_dataset[pbsy_dataset['year'] == 2012]\n",
    "entries_2012_null = pbsy_dataset[(pbsy_dataset['year'] == 2012) & (pbsy_dataset['povertyPercentage'].isnull())]\n",
    "\n",
    "print('- Amount of entries for 2012: ' + str(entries_2012.shape[0]))\n",
    "print('- Empty Amount of entries for 2012: ' + str(entries_2012_null.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to fix this problem, we will use the *mean imputation* technique including some random noise, which consist on computing the mean of the observed values for each variable and imputing the missing values for that variable by this mean, modifying it slightly for each case.\n",
    "\n",
    "In this case, instead of getting the complete average, we impute the missing values using the average of the adjacent years (2011 and 2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Value to control the strength of the noise\n",
    "noise_strength = 0.05\n",
    "\n",
    "# Set the entries of 2012 as missing rows\n",
    "missing_rows  = entries_2012_null\n",
    "\n",
    "# Get the average of 'povertyPercentage' for the adjacent years\n",
    "average_2011 = pbsy_dataset.loc[pbsy_dataset['year'] == 2011].groupby('state')['povertyPercentage'].transform('mean')\n",
    "average_2013 = pbsy_dataset.loc[pbsy_dataset['year'] == 2013].groupby('state')['povertyPercentage'].transform('mean')\n",
    "\n",
    "# If there are missing rows, calculate the average with added random noise\n",
    "if not missing_rows.empty:\n",
    "    # Calculate the average with added random noise\n",
    "    noisy_average = np.random.uniform((1 - noise_strength) * average_2011, (1 + noise_strength) * average_2013)\n",
    "\n",
    "    # Fill the missing values with the calculated averages with noise\n",
    "    pbsy_dataset.loc[missing_rows.index, 'povertyPercentage'] = noisy_average\n",
    "\n",
    "\n",
    "# Check if there are any missing values remaining\n",
    "entries_2012_null = pbsy_dataset[(pbsy_dataset['year'] == 2012) & (pbsy_dataset['povertyPercentage'].isnull())]\n",
    "print('- Empty Amount of entries for 2012: ' + str(entries_2012_null.shape[0]) + '\\n')\n",
    "\n",
    "#Print some rows as example\n",
    "entries_2012 = pbsy_dataset[pbsy_dataset['year'] == 2012]\n",
    "print(entries_2012.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we repeat the previous graph we can see that there are no missing values in 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=pbsy_dataset, x='year', y='povertyPercentage', hue='year', palette='viridis', dodge=False)\n",
    "plt.title('Poverty Percentage Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Poverty Percentage')\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, with all the modifications done, we will see the general distribution of the poverty rates throughout all the US. We can see that even tough there are some rates that are higher, the distribution is skewed right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=pbsy_dataset, x='povertyPercentage', bins=20, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Poverty Percentage')\n",
    "plt.xlabel('Poverty Percentage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the modifications we will create the two datasets with and without the \"United States\" as state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with 'United States' as state in pbsy_dataset\n",
    "pbsy_dataset_only_states = pbsy_dataset[pbsy_dataset['state'] != 'United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of elections dataset\n",
    "\n",
    "Similar analysis will be performed for the elections dataset. Firstly, we will check the evolution of the amount of votes per year. In general, we can say that the participation in the elections has been increasing by the years.\n",
    "\n",
    "Nevertheless, we know that the elections are every 4 years, while in this dataset there are entries every 2 years. That's why in the graph we can see the difference in the amount of votes every two years. We assume that this votes correspond to the midterm that the US does for the elections. For now we will keep it until concluding with the modifications. After finishing with the modifications we will separate in two datasets: one with all the voting campaigns and the other one just with the elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "-5fgfanTJDuv",
    "outputId": "95f0aee2-64e3-4c9e-d327-2786ae4188d3"
   },
   "outputs": [],
   "source": [
    "yearly_total_votes = ysdh_dataset.groupby('year')['totalvotes'].sum().reset_index(name='total_votes')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=yearly_total_votes, x='year', y='total_votes', hue='year', palette='flare')\n",
    "plt.title('Total Votes by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Votes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the scatter plot of the relations between candidate votes and total votes, we can detect some anomalies in the dataset. We have many values in the (0,0), which means that `candidatevotes` and `totalvotes` are the same and also we can see points in the top right of the chart with huge values, which are clearly anomalies as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='candidatevotes', y='totalvotes', data=ysdh_dataset)\n",
    "plt.title('Scatter Plot of Candidate Votes vs Total Votes')\n",
    "plt.xlabel('Candidate Votes')\n",
    "plt.ylabel('Total Votes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix that we calculate the Interquartile Range (IQR) for both `candidatevotes` and `totalvotes`, a measure of statistical dispersion based on quartiles, which is used to define upper and lower bounds for identifying outliers. Quartiles divide a dataset into four equal parts. The first quartile (Q1) represents the 25th percentile, and the third quartile (Q3) represents the 75th percentile. The IQR is the range between Q1 and Q3. Outliers were identified by defining bounds outside 1.5 times the IQR from the quartiles. This method is effective for detecting data points that deviate significantly from the central tendency of the dataset.\n",
    "\n",
    "Those entires that deviate from the tendency have been removed from the main dataset, to ensure that our subsequent analyses and visualizations are not influenced by extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the IQR for 'candidatevotes' and 'totalvotes'\n",
    "Q1_candidate = ysdh_dataset['candidatevotes'].quantile(0.25)\n",
    "Q3_candidate = ysdh_dataset['candidatevotes'].quantile(0.75)\n",
    "IQR_candidate = Q3_candidate - Q1_candidate\n",
    "\n",
    "Q1_total = ysdh_dataset['totalvotes'].quantile(0.25)\n",
    "Q3_total = ysdh_dataset['totalvotes'].quantile(0.75)\n",
    "IQR_total = Q3_total - Q1_total\n",
    "\n",
    "# Define the upper and lower bounds for outliers\n",
    "lower_bound_candidate = Q1_candidate - 1.5 * IQR_candidate\n",
    "upper_bound_candidate = Q3_candidate + 1.5 * IQR_candidate\n",
    "\n",
    "lower_bound_total = Q1_total - 1.5 * IQR_total\n",
    "upper_bound_total = Q3_total + 1.5 * IQR_total\n",
    "\n",
    "# Identify outliers\n",
    "outliers = ((ysdh_dataset['candidatevotes'] < lower_bound_candidate) | \n",
    "            (ysdh_dataset['candidatevotes'] > upper_bound_candidate) |\n",
    "            (ysdh_dataset['totalvotes'] < lower_bound_total) |\n",
    "            (ysdh_dataset['totalvotes'] > upper_bound_total))\n",
    "\n",
    "# Print the number of outliers\n",
    "print(\"In the dataset there are \" + str(ysdh_dataset[outliers].shape[0]) + \" outliers\")\n",
    "print(ysdh_dataset[outliers].head())\n",
    "\n",
    "# Remove outliers\n",
    "ysdh_dataset = ysdh_dataset[~outliers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='candidatevotes', y='totalvotes', data=ysdh_dataset)\n",
    "plt.title('Scatter Plot of Candidate Votes vs Total Votes')\n",
    "plt.xlabel('Candidate Votes')\n",
    "plt.ylabel('Total Votes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from that the big problem that we have with this dataset is its structure. The elections results are divided by states, which it is correct, but also by congressional districts, which is not relevant for our analysis.\n",
    "\n",
    "Moreover, we do not have the information about the amount of votes that each party obtained in each congressional district. We only know which party was the winner in each congressional party, and how many votes they got. Then we have the total amount of votes, but we do not know how are distributed among the rest of the parties.\n",
    "\n",
    "As an example, we will get the first year and the first state (1976 and Alabama):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_1976_alabama = ysdh_dataset[(ysdh_dataset['year'] == 1976) & (ysdh_dataset['state'] == 'ALABAMA')]\n",
    "print(rows_1976_alabama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alabama_1976_d1 = rows_1976_alabama.iloc[0]\n",
    "\n",
    "print('- Votes for Republicans in Alabama district 1: ' + str(alabama_1976_d1['candidatevotes']))\n",
    "print('- Votes for other parties in Alabama district 1: ' + str(alabama_1976_d1['totalvotes'] - alabama_1976_d1['candidatevotes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the US mainly all the votes goes for republicans or democrats, it could be said that the other votes, could be set to the other party in each row. However, we check that the dataset has more parties apart from republicans and democrats, so this modification would not be entirely correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_parties = ysdh_dataset['party'].unique()\n",
    "print(\"Different Parties:\", unique_parties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, we checked which is the percentage that these little parties occupy in the results. The result was less than a 1% of the entire votes during the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_votes_by_party = ysdh_dataset.groupby('party')['totalvotes'].sum().reset_index()\n",
    "\n",
    "# Calculate the percentage of total votes for each party\n",
    "total_votes_by_party['percentage'] = (total_votes_by_party['totalvotes'] / total_votes_by_party['totalvotes'].sum()) * 100\n",
    "\n",
    "print(total_votes_by_party)\n",
    "\n",
    "# Filter out Republicans and Democrats\n",
    "other_parties = total_votes_by_party[~total_votes_by_party['party'].isin(['REPUBLICAN', 'DEMOCRAT'])]\n",
    "\n",
    "# Calculate the sum of the percentage for other parties\n",
    "sum_percentage_other_parties = other_parties['percentage'].sum()\n",
    "\n",
    "print(f\"\\nSum of the percentage for other parties: {sum_percentage_other_parties:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we decide to remove these parties and just have the republicans and democrats.\n",
    "\n",
    "- *FOGLIETTA (DEMOCRAT)* will convert in *DEMOCRAT*\n",
    "- *DEMOCRATIC-FARMER-LABOR* will convert in DEMOCRAT\n",
    "- *INDEPENDENT* will convert randomly in *DEMOCRAT* or *REPUBLICAN*\n",
    "- *INDEPENDENT-REPUBLICAN* will convert in *REPUBLICAN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Update 'party' column\n",
    "ysdh_dataset['party'] = np.where(ysdh_dataset['party'] == 'FOGLIETTA (DEMOCRAT)', 'DEMOCRAT', ysdh_dataset['party'])\n",
    "ysdh_dataset['party'] = np.where(ysdh_dataset['party'] == 'DEMOCRATIC-FARMER-LABOR', 'DEMOCRAT', ysdh_dataset['party'])\n",
    "ysdh_dataset['party'] = np.where(ysdh_dataset['party'] == 'INDEPENDENT-REPUBLICAN', 'REPUBLICAN', ysdh_dataset['party'])\n",
    "\n",
    "# Randomly assign 'DEMOCRAT' or 'REPUBLICAN' for 'INDEPENDENT'\n",
    "independent_indices = ysdh_dataset[ysdh_dataset['party'] == 'INDEPENDENT'].index\n",
    "ysdh_dataset.loc[independent_indices, 'party'] = np.random.choice(['DEMOCRAT', 'REPUBLICAN'], size=len(independent_indices))\n",
    "\n",
    "# Verify the changes\n",
    "print(ysdh_dataset['party'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having just the two main parties, the first modification of the dataset will be to include two new columns: `republican_votes` and `democrat_votes`.\n",
    "\n",
    "If the party is republican, we know that `candidatevotes` are republican votes, so the rest are democrats votes (`totalvotes` - `candidatevotes`), and the same in the opposite way. In order to take into account the remaining votes for other political parties, we have reduced the calculation of the remaining votes to 1% (which is the average obtained previously, 0.87%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_votes(row):\n",
    "    resting_coef = 0.01\n",
    "    if row['party'] == 'REPUBLICAN':\n",
    "        return pd.Series([row['candidatevotes'], round((row['totalvotes'] - row['candidatevotes']) * (1 - resting_coef))])\n",
    "    else:\n",
    "        return pd.Series([round((row['totalvotes'] - row['candidatevotes']) * (1 - resting_coef)), row['candidatevotes']])\n",
    "\n",
    "# Apply the custom function to create new columns\n",
    "ysdh_dataset[['republican_votes', 'democrat_votes']] = ysdh_dataset.apply(calculate_votes, axis=1)\n",
    "\n",
    "# Print the updated dataset\n",
    "print(ysdh_dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this modification we can remove the `candidatevotes` columns, because it is useless for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'candidatevotes' column\n",
    "ysdh_dataset.drop('candidatevotes', axis=1, inplace=True)\n",
    "\n",
    "# Print the updated dataset\n",
    "print(ysdh_dataset[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude with the modifications, we will remove the congressional_district information and merge the rows that have the same year and state. This way, we will have the information about the votes that each party has obtained in each year and state, which has been the winner, and the total amount of votes pero party and in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'year' and 'state', summing up the votes\n",
    "voting_grouped_data = ysdh_dataset.groupby(['year', 'state']).agg({\n",
    "    'totalvotes': 'sum',\n",
    "    'republican_votes': 'sum',\n",
    "    'democrat_votes': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Update the 'party' column based on the comparison of votes\n",
    "voting_grouped_data['party'] = voting_grouped_data.apply(lambda row: 'REPUBLICAN' if row['republican_votes'] > row['democrat_votes'] else 'DEMOCRAT', axis=1)\n",
    "\n",
    "# Print the updated dataset\n",
    "print(voting_grouped_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that me conclude with the fixing, we will separate the dataset in two: one with all the voting campaigns and the other one just with the elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midterm_dataset = voting_grouped_data[voting_grouped_data['year'] % 4 != 0]\n",
    "elections_dataset = voting_grouped_data[voting_grouped_data['year'] % 4 == 0]\n",
    "\n",
    "# Display or print the first few rows to verify\n",
    "print(\"Mid term voting dataset:\")\n",
    "print(midterm_dataset['year'].unique())\n",
    "\n",
    "print(\"\\nElections dataset:\")\n",
    "print(elections_dataset['year'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total votes for each party per year\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=elections_dataset, x='year', y='totalvotes', hue='party', palette={'DEMOCRAT': 'blue', 'REPUBLICAN': 'red'}, errorbar=None)\n",
    "plt.title('Elections results Per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Votes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSi50038DAmp"
   },
   "source": [
    "#### Evolution of incidents over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolution of the incidents over time is one of the most relevant topics to analyze, so in the next graph we display it. We can see something strange. There are values that start around 2014, that finish around middle of 2018, and then values that restart in 2028. We don't know why that data is in the dataset, if by mistake or if it is some kind of prediction that they have made, where they have the incident data that they would like to have for the future. Whatever it is, for our analysis the future data is not relevant, so we are going to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "lWTjxMlL5l7v",
    "outputId": "3e18774f-a560-498f-bed8-48acc886b649"
   },
   "outputs": [],
   "source": [
    "# Count the number of incidents for each date\n",
    "incident_counts = incidents_dataset['date'].value_counts().sort_index()\n",
    "\n",
    "# Plotting the evolution of the amount of incidents over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=incident_counts.index, y=incident_counts.values, marker='o', color='skyblue')\n",
    "plt.title('Evolution of Incidents Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with 'date' in the future\n",
    "incidents_dataset = incidents_dataset[incidents_dataset['date'].dt.year < 2028]\n",
    "\n",
    "# Verify the changes\n",
    "print(incidents_dataset['date'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of incidents for each date\n",
    "incident_counts = incidents_dataset['date'].value_counts().sort_index()\n",
    "\n",
    "# Plotting the evolution of the amount of incidents over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=incident_counts.index, y=incident_counts.values, marker='o', color='skyblue')\n",
    "plt.title('Evolution of Incidents Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next issue that we can see is the difference between the incidents until 2014, with the incidents between 2014 and 2018. This does not occur because 2013 was the safest year of the history, but because there are way less values in the dataset than in the rest of the years. Therefore, in order not to alter the results, we have decided to remove entries older than 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the 'date' column and create a new 'year' column\n",
    "incidents_dataset['year'] = incidents_dataset['date'].dt.year\n",
    "\n",
    "# Count the number of incidents by year\n",
    "incident_counts_by_year = incidents_dataset['year'].value_counts().sort_index()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Number of Incidents by Year:\")\n",
    "print(incident_counts_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with 'date' in the future\n",
    "incidents_dataset = incidents_dataset[incidents_dataset['date'].dt.year > 2013]\n",
    "\n",
    "# Verify the changes\n",
    "print(incidents_dataset['date'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final result of the plot would be the next one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of incidents for each date\n",
    "incident_counts = incidents_dataset['date'].value_counts().sort_index()\n",
    "\n",
    "# Plotting the evolution of the amount of incidents over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=incident_counts.index, y=incident_counts.values, marker='o', color='skyblue')\n",
    "plt.title('Evolution of Incidents Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Incidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gO0nqrWDDdQ"
   },
   "source": [
    "#### Geographical distribution of incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "do5wMuMk95US",
    "outputId": "edd8beb6-58e4-4811-b11d-39b646ab0e36"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "world_filepath = './110m_cultural/ne_110m_admin_0_countries.shp'\n",
    "world = gpd.read_file(world_filepath)\n",
    "\n",
    "# Create a GeoDataFrame from the incidents_dataset DataFrame\n",
    "gdf = gpd.GeoDataFrame(incidents_dataset, geometry=gpd.points_from_xy(incidents_dataset.longitude, incidents_dataset.latitude))\n",
    "\n",
    "# Plot the world map\n",
    "world.plot(figsize=(12, 8), color='lightgrey', edgecolor='black')\n",
    "\n",
    "# Plot the scatter plot on top of the world map\n",
    "scatter_plot = sns.scatterplot(x='longitude', y='latitude', data=incidents_dataset, hue='n_killed', palette='viridis',\n",
    "                               size='n_injured', sizes=(20, 200), alpha=0.7)\n",
    "\n",
    "# Move the legend outside the chart\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title('Geographical Distribution of Incidents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geographical data fixing\n",
    "\n",
    "As we see in the data understanding section, there were some outliers in the geographical data, since it was visible that there were some incidents around India, when there would be just incidents from the USA.\n",
    "\n",
    "To fix that we refined the filtering criteria based on the latitude and longitude values. We set the latitude and longitude ranges to cover the area of the United States, including Alaska and Hawaii that have different coordinates that the main US region. These adjusted ranges ensure that incidents falling within the geographical coordinates of the entire United States are retained in the filtered dataset. This refined filtering approach allows for the accurate representation of incident locations on the geographical scatter plot while eliminating data points located outside the intended area of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming the latitude and longitude ranges for the United States\n",
    "us_latitude_range = (18, 71)\n",
    "us_longitude_range = (-179, -66)\n",
    "\n",
    "# Filter out incidents outside the US coordinates\n",
    "incidents_dataset_us = incidents_dataset[(incidents_dataset['latitude'].between(*us_latitude_range)) &\n",
    "                        (incidents_dataset['longitude'].between(*us_longitude_range))]\n",
    "\n",
    "# Create a GeoDataFrame from the filtered dataset\n",
    "gdf_us = gpd.GeoDataFrame(incidents_dataset_us, geometry=gpd.points_from_xy(incidents_dataset_us.longitude, incidents_dataset_us.latitude))\n",
    "\n",
    "# Plot the world map\n",
    "world_filepath = './110m_cultural/ne_110m_admin_0_countries.shp'\n",
    "world.plot(figsize=(12, 8), color='lightgrey', edgecolor='black')\n",
    "\n",
    "# Plot the scatter plot on top of the world map for the US incidents\n",
    "scatter_plot = sns.scatterplot(x='longitude', y='latitude', data=incidents_dataset_us, hue='n_killed', palette='viridis',\n",
    "                               size='n_injured', sizes=(20, 200), alpha=0.7)\n",
    "\n",
    "# Move the legend outside the chart\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title('Geographical Distribution of Incidents in the US')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEPm1u6oDHHZ"
   },
   "source": [
    "#### Distribution of participant age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the characteristics of the participants in the incidents it is one of the keys of our analysis, so we will start with the age of the participants. In the next graph we can have a preview of the distribution of the ages. We can see that there are ages that are greater than 100 years, including an entry with 311 years which is the maximum. This is almost impossible, so we will remove the values that have participants older than 110 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "RnwE42iZ7h82",
    "outputId": "abaadc08-db79-4392-bf57-4d512a789d36"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=incidents_dataset, x='participant_age1', bins=20, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Participant Ages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove participants older than 110 years\n",
    "incidents_dataset = incidents_dataset[incidents_dataset['participant_age1'] <= 110]\n",
    "\n",
    "# Verify the changes\n",
    "print(incidents_dataset['participant_age1'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=incidents_dataset, x='participant_age1', bins=20, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Participant Ages')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of participant gender\n",
    "\n",
    "Then we will analyze the gender of the participants. There are male and female values, but we also have an entry with the gender 'Male, female'. Since it is just one entry, and to simplify the analysis, we will remove this entry, converting in to \"Male\" and just have female and male categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOYoXyz_C7Kx"
   },
   "outputs": [],
   "source": [
    "gender_counts = incidents_dataset['participant_gender1'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Gender Distribution:\")\n",
    "print(gender_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with 'participant_gender1' equal to \"Male, female\"\n",
    "# Replace the column values where 'participant_gender1' is \"Male, female\" with \"Male\"\n",
    "incidents_dataset['participant_gender1'] = incidents_dataset['participant_gender1'].replace(\"Male, female\", \"Male\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "MfgOn-a1BjwD",
    "outputId": "ef95f56e-82a2-4407-f511-bcb8e8930cbc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=incidents_dataset, x='participant_gender1', palette='flare', hue='participant_gender1', legend=False)\n",
    "plt.title('Distribution of Participant Genders')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG8B0rj4EKy0"
   },
   "source": [
    "#### Number of Participants and Casualties\n",
    "\n",
    "In the next graph we show the distribution of the number of participants, and the number of injured, killed, arrested and unharmed participants. Even tough there are some outliers, the distribution is skewed to the right, since in most cases the number of participants is low and, on the contrary, there are very few cases with a large number of participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "TTTH_l3Z7iAG",
    "outputId": "22c9664f-01e9-4bf6-b9ef-8e28774e45ad"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=incidents_dataset[['n_participants', 'n_killed', 'n_injured','n_arrested', 'n_unharmed']])\n",
    "plt.title('Number of Participants and Casualties')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YDR2SvcEF5U"
   },
   "source": [
    "#### Incident Characteristics\n",
    "\n",
    "In the next graph the top 10 most occurred incidents will be displayed, to see which where the main ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "vtndoHUx7teE",
    "outputId": "b7406278-1fd1-424e-a23b-aa53c58fc5a9"
   },
   "outputs": [],
   "source": [
    "# Get the total number of different incident characteristics and print them\n",
    "total_characteristics = incidents_dataset['incident_characteristics1'].nunique()\n",
    "print('- Number of different incident characteristics: ' + str(total_characteristics))\n",
    "\n",
    "# Get the top 10 most common incident characteristics\n",
    "top10_characteristics = incidents_dataset['incident_characteristics1'].value_counts().nlargest(10).index\n",
    "\n",
    "# Create a countplot with the top 10 incident characteristics\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=incidents_dataset, y='incident_characteristics1', order=top10_characteristics, palette='muted', hue='incident_characteristics1', legend=False)\n",
    "plt.title('Top 10 Most Common Incident Characteristics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise correlation\n",
    "\n",
    "To perform pairwise correlation analysis, `corr()` function of `pandas` can be used, to compute the correlation matrix and then visualize it using a heatmap.\n",
    "\n",
    "The goal of pairwise correlation analysis is to understand the linear relationship between pairs of variables. The correlation coefficient ranges from -1 to 1, where:\n",
    "\n",
    "- 1 indicates a perfect positive correlation.\n",
    "- -1 indicates a perfect negative correlation.\n",
    "- 0 indicates no correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pairwise correlation matrix of the incidents dataset we do not have many interesting correlations to consider. We can observe a strong positive correlation with all the variables that measure the age, but that is obvious so it is not considerable. The most interesting fact could be the correlation between the incidents consequences and the gender. There is a stronger correlation between the consequences (`n_killed`, `n_arrested`, `n_injured`...) and  males than females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns for correlation analysis\n",
    "numerical_columns = incidents_dataset.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numerical_columns.corr()\n",
    "\n",
    "# Plot a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Pairwise Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incidents and elections correlation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the correlation matrix between the incidents and the elections, we can not say that there is any correlation to consider, since all the values are around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns of interest in incidents dataset\n",
    "incidents_columns_of_interest = [\n",
    "    'participant_age1', 'avg_age_participants', 'n_males', 'n_females', \n",
    "    'n_participants_child', 'n_participants_teen', 'n_killed', 'n_injured', 'n_arrested'\n",
    "]\n",
    "\n",
    "# Merge datasets on common keys\n",
    "merged_dataset = pd.merge(\n",
    "    incidents_dataset[incidents_columns_of_interest],\n",
    "    ysdh_dataset,\n",
    "    left_on=[incidents_dataset['state'], incidents_dataset['date'].dt.year],\n",
    "    right_on=['state', 'year']\n",
    ")\n",
    "\n",
    "# Select numerical columns for correlation analysis\n",
    "numerical_columns_merged = merged_dataset.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix_merged = numerical_columns_merged.corr()\n",
    "\n",
    "# Plot a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(11, 7))\n",
    "sns.heatmap(correlation_matrix_merged, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incidents and poverty rates correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the incidents and poverty rates, there is no clear correlation either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on common keys\n",
    "merged_dataset = pd.merge(\n",
    "    incidents_dataset[incidents_columns_of_interest],\n",
    "    pbsy_dataset_only_states,\n",
    "    left_on=[incidents_dataset['state'], incidents_dataset['date'].dt.year],\n",
    "    right_on=['state', 'year']\n",
    ")\n",
    "\n",
    "# Select numerical columns for correlation analysis\n",
    "numerical_columns_merged = merged_dataset.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix_merged = numerical_columns_merged.corr()\n",
    "\n",
    "# Plot a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(11, 7))\n",
    "sns.heatmap(correlation_matrix_merged, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poverty rates and elections correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the correlation between poverty rates and elections, even tough there is no a really clear and obvious correlation, we can observe an interesting difference between the correlation of the poverty percentage and the political party votes.\n",
    "\n",
    "The poverty percentage has a negative correlation of -0.34 with democrat votes, while the correlation with republican votes is almost zero, -0.06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on common keys\n",
    "merged_dataset = pd.merge(\n",
    "    ysdh_dataset,\n",
    "    pbsy_dataset_only_states,\n",
    "    left_on=['state', 'year'],\n",
    "    right_on=['state', 'year']\n",
    ")\n",
    "\n",
    "# Select numerical columns for correlation analysis\n",
    "numerical_columns_merged = merged_dataset.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix_merged = numerical_columns_merged.corr()\n",
    "\n",
    "# Plot a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(11, 7))\n",
    "sns.heatmap(correlation_matrix_merged, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making many modifications to the datasets, to conclude the data preparation aspect, we are going to improve the quality of the data, increasing the knowledge about it in order to extract new interesting features to describe the incidents. To do so, we are going to obtain several indicators for an incident by evaluating multiple aspects.\n",
    "\n",
    "The first question that has been addressed is the following:\n",
    "\n",
    "*How many males are involved in the incident w.r.t. the total number of males involved in incidents for the same city and in the same period?*\n",
    "\n",
    "In this case the period that we will analyze will be a year, so first, we add a new column representing the year extracted from the 'date' column. Subsequently, we group the dataset by city and year, calculating the total number of males involved in incidents for each combination. The total male information is then merged back into the original dataset (`n_males_total` column). To ensure accurate proportions in the subsequent calculations, any NaN or zero values in the total male column are replaced with 1. The function proceeds to compute the proportion of males involved in each incident (`proportion_males`), considering the total numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the year\n",
    "incidents_dataset['year'] = incidents_dataset['date'].dt.year\n",
    "\n",
    "# Group by city and year, calculate total males involved in incidents\n",
    "total_males_per_city_year = incidents_dataset.groupby(['city_or_county', 'year'])['n_males'].sum().reset_index()\n",
    "\n",
    "# Merge total males information back into the original dataset\n",
    "incidents_with_total_males = pd.merge(\n",
    "    incidents_dataset,\n",
    "    total_males_per_city_year,\n",
    "    on=['city_or_county', 'year'],\n",
    "    suffixes=('', '_total')\n",
    ")\n",
    "\n",
    "# Replace NaN or zero values in 'n_males_total' with 1 to avoid division issues\n",
    "incidents_with_total_males['n_males_total'].replace({0: 1}, inplace=True)\n",
    "\n",
    "# Calculate the proportion of males involved in each incident\n",
    "incidents_with_total_males['proportion_males'] = (\n",
    "    incidents_with_total_males['n_males'] / incidents_with_total_males['n_males_total']\n",
    ")\n",
    "\n",
    "# Display the result (you can adjust the columns displayed as needed)\n",
    "result_columns = ['date', 'city_or_county', 'n_males', 'n_males_total', 'proportion_males', 'incident_characteristics1']\n",
    "result = incidents_with_total_males[result_columns].sort_values(by=['city_or_county', 'date'])\n",
    "\n",
    "# Display the result to check if the proportions are correct\n",
    "print(result.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous modified dataset we can see the information of the proportion of males in incidents in a specific city. For testing we set manually the name of the city, but the user can modify it including the name of the desired city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by date\n",
    "incidents_with_total_males = incidents_with_total_males.sort_values(by='date')\n",
    "\n",
    "# Plot the proportion of males over time for a specific city\n",
    "# (For testing we specify manually the name of the city, but in production, user will be able to specify the city that he wants)\n",
    "city_to_plot = 'Boise'\n",
    "#city_to_plot = input('Introduce the name of a city: ')\n",
    "\n",
    "# Filter data for the selected city\n",
    "city_data = incidents_with_total_males[incidents_with_total_males['city_or_county'] == city_to_plot]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(city_data['date'], city_data['proportion_males'] * 100, linestyle='-', color='g')\n",
    "plt.title(f'Proportion of Males Involved in Incidents Over Time - {city_to_plot}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Proportion of Males')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next question was the next one:\n",
    "\n",
    "*How many injured and killed people have been involved w.r.t the total injured and killed people in the same congressional district in a given period of time?*\n",
    "\n",
    "The approach to achieve the result of this question has been similar as the previous one. To start, we have grouped the incidents dataset based on the congressional district and year, calculating the total number of individuals injured and killed in incidents for each combination. This information is then merged back into the original dataset. As it was done in the previous case, any NaN or zero values in the total injured and killed columns are replaced with 1. The function proceeds to compute the proportion of injured and killed people involved in each incident, considering the total numbers. With this data, the new dataset has 4 more columns: `n_injured_total`, `n_killed_total`, `proportion_injured` and `proportion_killed`. Finally, the dataset is filtered for a specific congressional district (specified as 1 for testing, but it can be set by the user), to analyze more detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by congressional district, year, and calculate total injured and killed people\n",
    "total_injured_killed_per_district_year = incidents_dataset.groupby(['congressional_district', 'year'])[['n_injured', 'n_killed']].sum().reset_index()\n",
    "\n",
    "# Merge total injured and killed information back into the original dataset\n",
    "incidents_with_total_injured_killed = pd.merge(\n",
    "    incidents_dataset,\n",
    "    total_injured_killed_per_district_year,\n",
    "    on=['congressional_district', 'year'],\n",
    "    suffixes=('', '_total')\n",
    ")\n",
    "\n",
    "# Replace NaN or zero values in 'n_injured_total' and 'n_killed_total' with 1 to avoid division issues\n",
    "incidents_with_total_injured_killed['n_injured_total'].replace({0: 1}, inplace=True)\n",
    "incidents_with_total_injured_killed['n_killed_total'].replace({0: 1}, inplace=True)\n",
    "\n",
    "# Calculate the proportion of injured and killed people involved in each incident\n",
    "incidents_with_total_injured_killed['proportion_injured'] = (\n",
    "    incidents_with_total_injured_killed['n_injured'] / incidents_with_total_injured_killed['n_injured_total']\n",
    ")\n",
    "incidents_with_total_injured_killed['proportion_killed'] = (\n",
    "    incidents_with_total_injured_killed['n_killed'] / incidents_with_total_injured_killed['n_killed_total']\n",
    ")\n",
    "\n",
    "# Filter data for a specific congressional district\n",
    "# (For testing we specify manually the number of the congressional district, but in production, user will be able to specify the number that he wants)\n",
    "congressional_district_to_plot = 1\n",
    "#congressional_district_to_plot = input('Introduce the number of the congressional district: ')\n",
    "\n",
    "\n",
    "# Filter data for the selected congressional district\n",
    "incidents_with_total_injured_killed_in_district = incidents_with_total_injured_killed[incidents_with_total_injured_killed['congressional_district'] == congressional_district_to_plot]\n",
    "\n",
    "print(incidents_with_total_injured_killed_in_district.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third part consists on the addition of new columns regarding to the ratios of participant and participants consequences in the incidents: killed, injured, arrested and unharmed people.\n",
    "\n",
    "To obtain this dataset, we make the calculation of the ratios using the required columns and inserting them in new columns. After including the columns, we have reduced the new dataset columns, to just the ones that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of killed people to participants in each incident\n",
    "incidents_with_participant_ratios = incidents_dataset.copy()\n",
    "incidents_with_participant_ratios['kill_participant_ratio'] = incidents_dataset['n_killed'] / incidents_dataset['n_participants']\n",
    "incidents_with_participant_ratios['injured_participant_ratio'] = incidents_dataset['n_injured'] / incidents_dataset['n_participants']\n",
    "incidents_with_participant_ratios['arrested_participant_ratio'] = incidents_dataset['n_arrested'] / incidents_dataset['n_participants']\n",
    "incidents_with_participant_ratios['unharmed_participant_ratio'] = incidents_dataset['n_unharmed'] / incidents_dataset['n_participants']\n",
    "\n",
    "# Display the result (you can adjust the columns displayed as needed)\n",
    "result_columns = ['date', 'state', 'city_or_county', 'n_killed', 'kill_participant_ratio',\n",
    "                  'n_injured', 'injured_participant_ratio', \n",
    "                  'n_arrested', 'arrested_participant_ratio',\n",
    "                  'n_unharmed', 'unharmed_participant_ratio',\n",
    "                  'n_participants', 'incident_characteristics1']\n",
    "incidents_with_participant_ratios = incidents_with_participant_ratios[result_columns].sort_values(by='date')\n",
    "\n",
    "# Print the result\n",
    "print(incidents_with_participant_ratios.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of visualization of the previous dataset, we have created the next plot, which shows the average ratios of the incident consequences for a specific city (As it done before, it is set manually for testing, but it can be customized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a specific city for filtering\n",
    "selected_city = 'Boise'\n",
    "#selected_city = input('Introduce the name of a city: ')\n",
    "\n",
    "# Filter data for the selected city\n",
    "city_data = incidents_with_participant_ratios[incidents_with_participant_ratios['city_or_county'] == selected_city]\n",
    "\n",
    "# Group by year and calculate the average ratios\n",
    "average_ratios_per_year = city_data.groupby(city_data['date'].dt.year)[\n",
    "    ['kill_participant_ratio', 'injured_participant_ratio', 'arrested_participant_ratio', 'unharmed_participant_ratio']\n",
    "].mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(9, 5))\n",
    "bar_width = 0.2\n",
    "bar_locations = range(len(average_ratios_per_year['date']))\n",
    "\n",
    "plt.bar(bar_locations, average_ratios_per_year['kill_participant_ratio'], width=bar_width, label='Kill Participant Ratio', color='r')\n",
    "plt.bar([i + bar_width for i in bar_locations], average_ratios_per_year['injured_participant_ratio'], width=bar_width, label='Injured Participant Ratio', color='b')\n",
    "plt.bar([i + 2 * bar_width for i in bar_locations], average_ratios_per_year['arrested_participant_ratio'], width=bar_width, label='Arrested Participant Ratio', color='g')\n",
    "plt.bar([i + 3 * bar_width for i in bar_locations], average_ratios_per_year['unharmed_participant_ratio'], width=bar_width, label='Unharmed Participant Ratio', color='purple')\n",
    "\n",
    "plt.title(f'Yearly Average Participant Ratios - {selected_city}')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Ratio')\n",
    "plt.xticks([i + 1.5 * bar_width for i in bar_locations], average_ratios_per_year['date'])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude with the exploration of new features we have calculated the average age of participants in incidents for each city or state to then examine how it changes over different periods.\n",
    "\n",
    "After creating the dataset with the average age participants of the dataset grouped by city and year, we display the results with three cities to compare. This way, in the plot we can see the difference between the evolution of participants average age over the different cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the year\n",
    "incidents_dataset['year'] = incidents_dataset['date'].dt.year\n",
    "\n",
    "# Group by city, state, and year, calculate the average age of participants\n",
    "average_age_per_city_year = incidents_dataset.groupby(['city_or_county', 'year'])['avg_age_participants'].mean().reset_index()\n",
    "\n",
    "# List of cities to compare\n",
    "cities_to_compare = ['San Francisco', 'Las Vegas', 'Orlando']\n",
    "\n",
    "# Filter data for the selected cities\n",
    "average_age_per_city_year = average_age_per_city_year[average_age_per_city_year['city_or_county'].isin(cities_to_compare)]\n",
    "\n",
    "# Plotting the average age over different periods for selected cities\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.lineplot(x='year', y='avg_age_participants', hue='city_or_county', data=average_age_per_city_year, marker='o')\n",
    "plt.title('Average Age of Participants in Incidents')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Age')\n",
    "plt.legend(title='City or County', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the new features obtained in the previous exploration, we add some modifications to the previous approach. Here the function groups the dataset by incident characteristic, city and year, and then calculates the average age of the participants. This dataset is filtered by desired incident types and cities. Then the result is shown in a graph where we can observe the differences in average ages of the different cities and incident types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by incident characteristics, city, and year, and calculate the average age of participants\n",
    "average_age_per_incident_type_city_year = incidents_dataset.groupby(['incident_characteristics1', 'city_or_county', 'year'])['avg_age_participants'].mean().reset_index()\n",
    "\n",
    "# List of incident types to compare\n",
    "incident_types_to_compare = ['Shot - Wounded/Injured', 'Non-Shooting Incident']\n",
    "\n",
    "# List of cities to compare\n",
    "cities_to_compare = ['San Francisco', 'Las Vegas', 'Orlando']\n",
    "\n",
    "# Filter data for the selected incident types and cities\n",
    "average_age_per_incident_type_city_year = average_age_per_incident_type_city_year[\n",
    "    (average_age_per_incident_type_city_year['incident_characteristics1'].isin(incident_types_to_compare)) &\n",
    "    (average_age_per_incident_type_city_year['city_or_county'].isin(cities_to_compare))\n",
    "]\n",
    "\n",
    "# Plotting the average age over different periods for selected incident types and cities\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='year', y='avg_age_participants', hue='city_or_county', \n",
    "             style='incident_characteristics1', data=average_age_per_incident_type_city_year, marker='o', errorbar=None)\n",
    "plt.title('Average Age of Participants by Types and Cities')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Age')\n",
    "plt.legend(title='Incident Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
